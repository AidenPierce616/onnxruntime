#include <cuda.h>
#include <stdint.h>
#include <assert.h>
#include "contrib_ops/cuda/sparse/sparse_attention_trition/sparse_attention_api.h"

namespace onnxruntime {
namespace contrib {
namespace cuda {

//--------------------------------------------------------------------------
// This session is auto-generated by compile_sparse_attention.py.
// To update this session, copy generated functions from sparse_attention_api_fp16_sm80.cc.
//--------------------------------------------------------------------------

// launcher for: sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2
Status sparse_attention_sm80_fp16_a94506bb(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_a94506bb(params);
}

// load for: sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2
void load_sparse_attention_sm80_fp16_a94506bb();
void load_sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2() {
  load_sparse_attention_sm80_fp16_a94506bb();
}

// unload for: sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2
void unload_sparse_attention_sm80_fp16_a94506bb();
void unload_sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2() {
  unload_sparse_attention_sm80_fp16_a94506bb();
}

// launcher for: sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2
Status sparse_attention_sm80_fp16_f1e7ed8e(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_f1e7ed8e(params);
}

// load for: sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2
void load_sparse_attention_sm80_fp16_f1e7ed8e();
void load_sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2() {
  load_sparse_attention_sm80_fp16_f1e7ed8e();
}

// unload for: sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2
void unload_sparse_attention_sm80_fp16_f1e7ed8e();
void unload_sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2() {
  unload_sparse_attention_sm80_fp16_f1e7ed8e();
}

// launcher for: sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2
Status sparse_attention_sm80_fp16_5d10b453(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_5d10b453(params);
}

// load for: sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2
void load_sparse_attention_sm80_fp16_5d10b453();
void load_sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2() {
  load_sparse_attention_sm80_fp16_5d10b453();
}

// unload for: sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2
void unload_sparse_attention_sm80_fp16_5d10b453();
void unload_sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2() {
  unload_sparse_attention_sm80_fp16_5d10b453();
}

// launcher for: sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2
Status sparse_attention_sm80_fp16_b286eb9c(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_b286eb9c(params);
}

// load for: sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2
void load_sparse_attention_sm80_fp16_b286eb9c();
void load_sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2() {
  load_sparse_attention_sm80_fp16_b286eb9c();
}

// unload for: sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2
void unload_sparse_attention_sm80_fp16_b286eb9c();
void unload_sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2() {
  unload_sparse_attention_sm80_fp16_b286eb9c();
}

// launcher for: sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2
Status sparse_attention_sm80_fp16_739da152(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_739da152(params);
}

// load for: sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2
void load_sparse_attention_sm80_fp16_739da152();
void load_sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2() {
  load_sparse_attention_sm80_fp16_739da152();
}

// unload for: sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2
void unload_sparse_attention_sm80_fp16_739da152();
void unload_sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2() {
  unload_sparse_attention_sm80_fp16_739da152();
}

// launcher for: sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2
Status sparse_attention_sm80_fp16_eb8740ba(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_eb8740ba(params);
}

// load for: sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2
void load_sparse_attention_sm80_fp16_eb8740ba();
void load_sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2() {
  load_sparse_attention_sm80_fp16_eb8740ba();
}

// unload for: sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2
void unload_sparse_attention_sm80_fp16_eb8740ba();
void unload_sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2() {
  unload_sparse_attention_sm80_fp16_eb8740ba();
}

// launcher for: sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2
Status sparse_attention_sm80_fp16_a105022b(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_a105022b(params);
}

// load for: sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2
void load_sparse_attention_sm80_fp16_a105022b();
void load_sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2() {
  load_sparse_attention_sm80_fp16_a105022b();
}

// unload for: sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2
void unload_sparse_attention_sm80_fp16_a105022b();
void unload_sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2() {
  unload_sparse_attention_sm80_fp16_a105022b();
}

// launcher for: sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2
Status sparse_attention_sm80_fp16_f4a89c6a(SparseAttentionParams& params);

Status sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2(SparseAttentionParams& params) {
  return sparse_attention_sm80_fp16_f4a89c6a(params);
}

// load for: sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2
void load_sparse_attention_sm80_fp16_f4a89c6a();
void load_sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2() {
  load_sparse_attention_sm80_fp16_f4a89c6a();
}

// unload for: sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2
void unload_sparse_attention_sm80_fp16_f4a89c6a();
void unload_sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2() {
  unload_sparse_attention_sm80_fp16_f4a89c6a();
}

typedef Status (*kernel_func_t)(SparseAttentionParams& params);
kernel_func_t sparse_attention_sm80_fp16_kernels[] = {
    sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2,
    sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2,
    sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2,
    sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2,
    sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2,
    sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2,
    sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2,
    sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2,
};

int sparse_attention_sm80_fp16_get_num_algos(void) {
  return (int)sizeof(sparse_attention_sm80_fp16_kernels);
}

Status sparse_attention_sm80_fp16(SparseAttentionParams& params, int algo_id) {
  assert(algo_id < (int)sizeof(sparse_attention_sm80_fp16_kernels));
  return sparse_attention_sm80_fp16_kernels[algo_id](params);
}

void load_sparse_attention_sm80_fp16(void) {
  load_sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2();
  load_sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2();
  load_sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2();
  load_sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2();
  load_sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2();
  load_sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2();
  load_sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2();
  load_sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2();
}

void unload_sparse_attention_sm80_fp16(void) {
  unload_sparse_attention_sm80_fp16_16x0x64x0x64x2_warps1xstages2();
  unload_sparse_attention_sm80_fp16_16x0x64x1x64x2_warps1xstages2();
  unload_sparse_attention_sm80_fp16_16x1x64x0x64x2_warps1xstages2();
  unload_sparse_attention_sm80_fp16_16x1x64x1x64x2_warps1xstages2();
  unload_sparse_attention_sm80_fp16_64x0x64x0x64x2_warps4xstages2();
  unload_sparse_attention_sm80_fp16_64x0x64x1x64x2_warps4xstages2();
  unload_sparse_attention_sm80_fp16_64x1x64x0x64x2_warps4xstages2();
  unload_sparse_attention_sm80_fp16_64x1x64x1x64x2_warps4xstages2();
}

// Status sparse_attention_sm80_fp16_default(SparseAttentionParams& params){
//   return sparse_attention_sm80_fp16(params, 0);
// }

//--------------------------------------------------------------------------
// End of auto-generated section.
// The following functions are added manually
//--------------------------------------------------------------------------

int get_algo_id(SparseAttentionParams& params) {
  int block_n = params.kernel_block_size;
  int block_m = block_n;
  bool even_m = (params.sequence_length % block_m == 0);
  bool even_n = (params.total_sequence_length % block_n == 0);

  if (params.head_size == 128) {
    if (block_m == 16) {
      if (!even_m) {
        return even_n ? 1 : 0;
      } else {
        return even_n ? 3 : 2;
      }
    } else if (block_m == 64) {
      if (!even_m) {
        return even_n ? 5 : 4;
      } else {
        return even_n ? 7 : 6;
      }
    }
  }

  return -1;
}

bool is_supported_sparse_attention(const cudaDeviceProp& dprops) {
  return dprops.major == 8;
}

bool is_supported_sparse_attention(int head_size, int sparse_block_size) {
  return head_size == 128 && sparse_block_size == 64;
}

Status run_sparse_attention_fp16(SparseAttentionParams& params) {
  int algo_id = get_algo_id(params);
  if (algo_id < 0) {
    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "no algo found for the parameters");
    ;
  }

  return sparse_attention_sm80_fp16(params, algo_id);
}

static std::once_flag load_sparse_attention_kernel_flag;

void load_sparse_attention_fp16(void) {
  std::call_once(load_sparse_attention_kernel_flag, load_sparse_attention_sm80_fp16);
}

void unload_sparse_attention_fp16(void) {
  unload_sparse_attention_sm80_fp16();
}

}  // namespace cuda
}  // namespace contrib
}  // namespace onnxruntime
